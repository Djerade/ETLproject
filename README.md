# ğŸ›’ Projet ETL E-commerce vers un Data Lakehouse

Ce projet implÃ©mente un pipeline **ETL** pour une plateforme e-commerce, en utilisant **Apache Spark**, **Delta Lake**, **Apache Airflow** et **Hive Metastore**. Lâ€™objectif est de consolider des donnÃ©es issues de plusieurs sources (CSV, JSON, MySQL) vers un **data lakehouse** structurÃ© pour des analyses avancÃ©es.

---

## ğŸ“¦ Objectifs du projet

- Extraire des donnÃ©es clients, produits et ventes.
- Nettoyer et transformer les donnÃ©es en un schÃ©ma en Ã©toile (star schema).
- Stocker les donnÃ©es en format **Delta** partitionnÃ© pour des performances optimales.
- Orchestrer les traitements avec **Airflow**.
- Mettre en place une base solide pour la BI ou le machine learning.

---

## ğŸ“ Structure du projet

/ecommerce-etl-pipeline/
â”œâ”€â”€ dags/ # DAGs Airflow
â”‚ â””â”€â”€ etl_pipeline.py
â”œâ”€â”€ config/
â”‚ â””â”€â”€ mysql_config.json # Config de connexion MySQL
â”œâ”€â”€ data/ # DonnÃ©es brutes
â”‚ â”œâ”€â”€ clients.csv
â”‚ â”œâ”€â”€ produits.json
â”‚ â””â”€â”€ ventes.csv
â”œâ”€â”€ spark_jobs/ # Scripts Spark
â”‚ â”œâ”€â”€ extract_transform.py
â”‚ â””â”€â”€ load_to_delta.py
â”œâ”€â”€ lakehouse/ # Data Lakehouse Delta
â”‚ â””â”€â”€ delta_tables/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ analyse_delta_lake.ipynb
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ Technologies utilisÃ©es

- [Apache Spark](https://spark.apache.org/)
- [Apache Airflow](https://airflow.apache.org/)
- [Delta Lake](https://delta.io/)
- [Hive Metastore](https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin)
- Python 3.10
- Docker / Docker Compose

---
